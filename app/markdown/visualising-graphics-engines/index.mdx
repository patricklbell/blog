Modern game engines such as Unity and Unreal have made 3D graphics more accessible than ever. In the process they have made it hard to know how things actually show up on the screen. How do games look more realistic now? To help answer this, I thought I would show each of the processes my own engine takes to create a frame.

# Materials

<Image src={`${props.assetsUrl}imgs/BASE.webp`} width={1912} height={996}>
  Rendering the scene with point and directional lighting only.
</Image>
The first step is rendering all the objects with their correct materials. This involves switching between shaders to draw different materials with different lighting models and making sure each material reacts correctly to the point and directional lights in the scene. I am skipping over a lot here because this is a topic which deserves a better explanation, [Google's Filament documentation](https://google.github.io/filament/Materials.html) is a great resources for understanding materials and lighting.

At this stage things looks either way too dark or way too bright, that's mainly because of tonemapping. Tonemapping is one of the last steps and it will transform the image from a high dynamic range (HDR) to something most screens can display.

# Shadows
<Image src={`${props.assetsUrl}imgs/SHADOWS.webp`} width={1912} height={996}>
  Base render with directional shadows added.
</Image>
To add shadows I sample a cascaded shadow map, this allows a shader to know if a point should be in shadow. Being in shadow in this case means that the directional light contribution is zero, because point lights don't cast shadows (shadows are very expensive so reducing the number of casters is important).

## Cascaded Shadow Map
<Image src={`${props.assetsUrl}imgs/CSM.webp`} width={240} height={240}>
  Nearest slice of the cascaded shadow map.
</Image>

The cascaded shadow map is a depth only pass which happens before rendering, it stores the relative depth of geometry from each directional light's perspective. A point can be projected into the light's view and it's depth compared with the depth map to determine if the point is in shadow. If you're curious, the [shadow shader](https://github.com/patricklbell/3d_engine/blob/569e867ce7fa31a6845a40d63169516a03f49e12/data/shaders/null.glsl) (ignore the vegetation, skeletal animation and clipping) and the [sampler](https://github.com/patricklbell/3d_engine/blob/569e867ce7fa31a6845a40d63169516a03f49e12/data/shaders/lib/shadows.glsl) which reduces aliasing from sampling the shadow map.

# Volumetrics
<Image src={`${props.assetsUrl}imgs/VOLUMETRICS.webp`} width={1912} height={996}>
  Addition of volumetric fog effects.
</Image>
Each point also samples an accumulated volumetric fog color which is added to the resulting color. This adds a sense of depth to the lighting by roughly approximating in-scattering due to particulates in the atmosphere (such as the small water droplets in fog). This has the byproduct of adding some extra ambient lighting.

## Volumetric Integration
<Image src={`${props.assetsUrl}imgs/INTEGRATED_VOLUME.webp`} width={160} height={90}>
  A slice of the raymarched volumetric 3D texture
</Image>
This accumulated color is computed by marching rays through the camera's frustrum, accumulating color and density in a low resolution 3D texture. The fog density is generated by noise here, but could also follow more advanced rules. This technique was created for EA's frostbite engine which they [documented](https://www.ea.com/frostbite/news/physically-based-unified-volumetric-rendering-in-frostbite) or you can see my compute shaders, which I broke into two steps: [volume integration](https://github.com/patricklbell/3d_engine/blob/569e867ce7fa31a6845a40d63169516a03f49e12/data/shaders/volumetric_integration.glsl) and [raymarching](https://github.com/patricklbell/3d_engine/blob/569e867ce7fa31a6845a40d63169516a03f49e12/data/shaders/volumetric_raymarch.glsl).

# Post Processing
Most of the effects I apply should be familiar to anyone who has opened 3D games' settings, so I am going to move quickly over them. All these post processing steps are fairly simple, if fiddly, and can be found in a [single shader](https://github.com/patricklbell/3d_engine/blob/569e867ce7fa31a6845a40d63169516a03f49e12/data/shaders/post.glsl) for my engine.

## Antialiasing
<Image src={`${props.assetsUrl}imgs/AA_diff.webp`} width={1912} height={996}>
  Contribution of fast Approximate Anti Aliasing (FXAA) and multisampling. Gamma and contrast have been adjusted to highlight the effect.
</Image>
Antialiasing reduces the jagged edges created during the rasterization process. FXAA is a screen space effect which was developed by NVIDIA which essentially blurs jagged edges. It is becoming less important with the development of temporal antialiasing techniques and deep learning based super-samplers (DLSS).

## Bloom
<Image src={`${props.assetsUrl}imgs/BLOOM_diff.webp`} width={1912} height={996}>
  Contribution of additive blending with a bloom texture.
</Image>

Bloom simulates the blurring that occurs when looking at bright lights. This effect is achieved by creating a blurred version of the screen texture and blending additively ($$\text{color}_{\text{bloom}} = \text{color}_{\text{screen}} + \text{color}_{\text{blurred}}$$). I use the classic progressive downsampling method after I foun it less expensive than a Gaussian blur with no noticeable quality loss.

## Ambient Occlusion
<Image src={`${props.assetsUrl}imgs/SSAO_diff.webp`} width={1912} height={996}>
  Contribution of screen space ambient occlusion, gamma has been increased and saturation decreased to highlight effect.
</Image>

Screen space ambient occlusion (SSAO) calculates regions with high concavity and adds extra shadows, I have kept this extremely subtle because this is mostly a failed experiment (I replaced SSAO with a lightmapper which should produces more physically accurate results but was too slow to generate for this scene).

## Exposure and Dynamic Range
<Image src={`${props.assetsUrl}imgs/EXPOSURE-TONEMAPPING-GAMMA.webp`} width={1912} height={996}>
  Image after eye exposure correction, tonemapping, and gamma correction.
</Image>

Eye exposure correction approximates the contraction and dilation of our pupils when it is very bright or dark, similar to how a film camera can automatically adjust it's exposure. Tonemapping compresses the floating point colors (HDR) we have been using into the 0-1 range of a monitor, Gamma correction then converts the linear brightness to a curve which fits the perceived brightness of the monitor's pixels.

## Effects
<Image src={`${props.assetsUrl}imgs/FINAL.webp`} width={1912} height={996}>
  A simple vignette which darkens the screen corners completes the image.
</Image>
